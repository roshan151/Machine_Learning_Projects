{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFWbEb6uGbN-"
   },
   "source": [
    "# Text Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BOwsuGQQY9OL",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZ4qOUzujMP6",
    "outputId": "50458685-9bde-4f5c-ca3e-12c16c786318",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#Download the sonnects file from this drive location or this repo. \n",
    "#sonnets = 'https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K'\n",
    "SONNETS_FILE = 'C:/Users/RTiwari1/sonnets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pfd-nYKij5yY",
    "outputId": "dfed55ca-5152-471c-a748-798390a99ffd",
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2159 lines of sonnets\n",
      "\n",
      "The first 5 lines look like this:\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou, contracted to thine own bright eyes,\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "with open('./sonnets.txt') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Convert to lower case and save as a list\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
    "print(f\"The first 5 lines look like this:\\n\")\n",
    "for i in range(5):\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AAhM_qAZk0o5",
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 417, 877, 166, 213, 517]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer fit on texts generates token number for all words in corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "#text to sequences uses token numbers to create sequence of each line\n",
    "tokenizer.texts_to_sequences([corpus[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iy4baJMDl6kj",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# This function breaks every line by removing one word every time.\n",
    "# The model will be trained to predict the next word everytime.\n",
    "def n_gram_seqs(corpus, tokenizer):\n",
    "    input_sequences = []\n",
    "    \n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    return input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlKqW2pfM7G3",
    "outputId": "8a274f8c-5871-4b8e-8cad-c3d63b2bf851",
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram sequences for first example look like this:\n",
      "\n",
      "padded corpus has shape: (15462, 11)\n"
     ]
    }
   ],
   "source": [
    "# Testing function with one example.\n",
    "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
    "\n",
    "print(\"n_gram sequences for first example look like this:\\n\")\n",
    "first_example_sequence\n",
    "\n",
    "# Apply the n_gram_seqs transformation to the whole corpus\n",
    "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
    "\n",
    "# Save max length \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad the corpus by adding 0 to start of shorter sequences.\n",
    "padded_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding = 'pre'))\n",
    "print(f\"padded corpus has shape: {padded_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0  34 417]\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "code",
    "id": "9WGGbYdnZdmJ",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#Divide sequences into features and labels by taking the last token as label and entire sequence except the last token as features.\n",
    "#Then one hot encode the labels using total words as classes. Only the correct word is one rest all are zero.\n",
    "def features_and_labels(input_sequences, total_words):\n",
    "\n",
    "    features = input_sequences[:,:-1]\n",
    "    labels = input_sequences[:,-1]\n",
    "\n",
    "    one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n",
    "    return features, one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRTuLEt3bRKa",
    "outputId": "a5cc53bc-64e4-4588-e27f-2052d4acd9d0",
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features have shape: (15462, 10)\n",
      "labels have shape: (15462, 3211)\n"
     ]
    }
   ],
   "source": [
    "# Split the whole corpus\n",
    "features, labels = features_and_labels(padded_sequences, total_words)\n",
    "\n",
    "print(f\"features have shape: {features.shape}\")\n",
    "print(f\"labels have shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "code",
    "id": "XrE6kpJFfvRY",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# create_model\n",
    "def create_model(total_words, max_sequence_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=None))\n",
    "    model.add(Bidirectional(LSTM(150)))\n",
    "    model.add(Dense(total_words, activation = 'softmax'))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    adam = Adam(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IpX_Gu_gISk",
    "outputId": "c26db44a-4621-473a-f829-c9f4865920af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RTiwari1\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "484/484 [==============================] - 15s 24ms/step - loss: 6.8810 - accuracy: 0.0244\n",
      "Epoch 2/50\n",
      "484/484 [==============================] - 11s 24ms/step - loss: 6.4322 - accuracy: 0.0311\n",
      "Epoch 3/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 6.1906 - accuracy: 0.0411\n",
      "Epoch 4/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 5.9241 - accuracy: 0.0534\n",
      "Epoch 5/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 5.6269 - accuracy: 0.0645\n",
      "Epoch 6/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 5.2865 - accuracy: 0.0737\n",
      "Epoch 7/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 4.9165 - accuracy: 0.0936\n",
      "Epoch 8/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 4.5335 - accuracy: 0.1196\n",
      "Epoch 9/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 4.1474 - accuracy: 0.1630\n",
      "Epoch 10/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 3.7729 - accuracy: 0.2230\n",
      "Epoch 11/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 3.4046 - accuracy: 0.2888\n",
      "Epoch 12/50\n",
      "484/484 [==============================] - 11s 22ms/step - loss: 3.0810 - accuracy: 0.3490\n",
      "Epoch 13/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 2.7812 - accuracy: 0.4084\n",
      "Epoch 14/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 2.5161 - accuracy: 0.4624\n",
      "Epoch 15/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 2.2794 - accuracy: 0.5169\n",
      "Epoch 16/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 2.0688 - accuracy: 0.5644\n",
      "Epoch 17/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 1.8768 - accuracy: 0.6064\n",
      "Epoch 18/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 1.7130 - accuracy: 0.6464\n",
      "Epoch 19/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 1.5586 - accuracy: 0.6769\n",
      "Epoch 20/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 1.4223 - accuracy: 0.7078\n",
      "Epoch 21/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 1.3073 - accuracy: 0.7321\n",
      "Epoch 22/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 1.2002 - accuracy: 0.7563\n",
      "Epoch 23/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 1.1064 - accuracy: 0.7754\n",
      "Epoch 24/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 1.0334 - accuracy: 0.7875\n",
      "Epoch 25/50\n",
      "484/484 [==============================] - 10s 22ms/step - loss: 0.9628 - accuracy: 0.8007\n",
      "Epoch 26/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 0.8999 - accuracy: 0.8131\n",
      "Epoch 27/50\n",
      "484/484 [==============================] - 11s 24ms/step - loss: 0.8538 - accuracy: 0.8188\n",
      "Epoch 28/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 0.8141 - accuracy: 0.8240\n",
      "Epoch 29/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 0.7803 - accuracy: 0.8278\n",
      "Epoch 30/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 0.7481 - accuracy: 0.8335\n",
      "Epoch 31/50\n",
      "484/484 [==============================] - 11s 22ms/step - loss: 0.7220 - accuracy: 0.8360\n",
      "Epoch 32/50\n",
      "484/484 [==============================] - 10s 21ms/step - loss: 0.7008 - accuracy: 0.8393\n",
      "Epoch 33/50\n",
      "484/484 [==============================] - 11s 24ms/step - loss: 0.6821 - accuracy: 0.8416\n",
      "Epoch 34/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 0.6610 - accuracy: 0.8430\n",
      "Epoch 35/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 0.6517 - accuracy: 0.8450\n",
      "Epoch 36/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.6407 - accuracy: 0.8450\n",
      "Epoch 37/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.6233 - accuracy: 0.8469\n",
      "Epoch 38/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 0.6146 - accuracy: 0.8479\n",
      "Epoch 39/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.6103 - accuracy: 0.8476\n",
      "Epoch 40/50\n",
      "484/484 [==============================] - 11s 23ms/step - loss: 0.6031 - accuracy: 0.8479\n",
      "Epoch 41/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.5994 - accuracy: 0.8477\n",
      "Epoch 42/50\n",
      "484/484 [==============================] - 13s 28ms/step - loss: 0.5923 - accuracy: 0.8473\n",
      "Epoch 43/50\n",
      "484/484 [==============================] - 14s 28ms/step - loss: 0.5854 - accuracy: 0.8496\n",
      "Epoch 44/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 0.5834 - accuracy: 0.8490\n",
      "Epoch 45/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.5770 - accuracy: 0.8498\n",
      "Epoch 46/50\n",
      "484/484 [==============================] - 11s 22ms/step - loss: 0.5726 - accuracy: 0.8494\n",
      "Epoch 47/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.5640 - accuracy: 0.8501\n",
      "Epoch 48/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 0.5777 - accuracy: 0.8476\n",
      "Epoch 49/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.5742 - accuracy: 0.8489\n",
      "Epoch 50/50\n",
      "484/484 [==============================] - 12s 24ms/step - loss: 0.5611 - accuracy: 0.8509\n"
     ]
    }
   ],
   "source": [
    "# Get the untrained model\n",
    "model = create_model(total_words, max_sequence_len)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(features, labels, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "1fXTEO3GJ282",
    "outputId": "6127b7d7-aedc-4beb-ed0c-8977c3c7a96f",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Analyzing the training curve of the model.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(acc))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Analyzing the training curve of the model.\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "9QRG73l6qE-c",
    "outputId": "a0f070eb-999f-4ef0-f607-28bf4d191c98",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_a54091aa-fcf3-4bfe-b042-8fd1b4a1a0b9\", \"history.pkl\", 944)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_history():\n",
    "    import pickle\n",
    "        from google.colab import files\n",
    "\n",
    "    with open('history.pkl', 'wb') as f:\n",
    "    \n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "        files.download('history.pkl')\n",
    "\n",
    "    download_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Vc6PHgxa6Hm",
    "outputId": "d9db622a-fb02-4328-c27b-a0cce60b46b1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me Obi Wan Kenobi, you're my only hope my argument all hate to thee all me see all other words thee all me or me more or less thee for love one thine doth mine pen truth again thee new mine eyes the write of heart that you right you had you dearer of another's account ' say so one of such you call so you die me do thee thy fair part thee so true thy 'will ' die in thee so best to thee i thine not thee thy joy can thee that die me still the heart i see the 'will ' die ' die\n"
     ]
    }
   ],
   "source": [
    "#Feed an input sequence and generate the next hundred words. \n",
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted = np.argmax(predicted, axis=-1).item()\n",
    "    output_word = tokenizer.index_word[predicted]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "C3W4_Assignment.ipynb",
   "provenance": []
  },
  "dlai_version": "1.2.0",
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
